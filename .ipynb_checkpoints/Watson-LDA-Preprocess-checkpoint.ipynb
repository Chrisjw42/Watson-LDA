{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import random\n",
    "from googleapiclient.discovery import build\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFb = pd.read_csv(\"data/facebookArticlesClean.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataG = pd.read_csv(\"data/googlePagesClean.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dataG[dataG.columns[2:]].append(dataFb[dataFb.columns[2:]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filtering(s):\n",
    "    s = s.lower()\n",
    "    s = ''.join([c for c in s if c in string.printable]) # Get rid of non ascii whitespace chars, e.g. japanese\n",
    "    s = s.strip() # Get rid of whitespace AFTER removing chars\n",
    "    return s\n",
    "\n",
    "data[\"text\"] = data[\"TRANSLATED_CONTENT\"].apply(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['official name codonopsis radix english name tangshen scientific name codonpsis pilosula franch nannf codonopsis pilosula nannf var modesta nannf l t shen or codonopsis tangshen oliv chinese read more',\n",
       " 'official name cinnamomi ramulus english name cassia twig scientific name cinnamomum cassia presl chinese name ?? chinese phonetic name gu zh?',\n",
       " 'official name angelicae dahuricae radix english name dahurian angelica root scientific name angelica dahurica fisch ex hoffm benth et hook f angelica dahurica fisch ex hoffm benth read more',\n",
       " 'official name fructus forsythiae english name weeping forsythia capsule scientific name forsythia suspensa thunb vahl chinese name ?? chinese phonetic name lian qiao source the dried fruit read more',\n",
       " 'benefits of yunzhi and lingzhi  there are approximately 100 thousand known mushroom species in the world which have been a rich medicinal reservoir that continues today read more']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(data.iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret string representation of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This entry was dodgy\n",
    "data.drop(733, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"text_list\"] = data[\"text\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop([\"CONTENT\", \"TRANSLATED_CONTENT\", \"FIRST_PARAGRAPH\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explode(s):\n",
    "    return \" \".join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"text_full\"] = data[\"text_list\"].apply(explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop(\"text\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All puncuation characters to be ignored\n",
    "exclude = set(string.punctuation)\n",
    "# In string form\n",
    "excludestr = ''.join(exclude)\n",
    "\n",
    "def splitcol(s):\n",
    "    s = ''.join([c for c in s if c not in exclude]) # Removes punctuation\n",
    "    s = [x for x in s.split(\" \") if x != \"\"] # Accounts for double whitespace\n",
    "    return s\n",
    "\n",
    "# Create a column that is every word, split. \n",
    "data[\"text_split\"] = data[\"text_full\"].apply(splitcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_list</th>\n",
       "      <th>text_full</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[official name codonopsis radix english name t...</td>\n",
       "      <td>official name codonopsis radix english name ta...</td>\n",
       "      <td>[official, name, codonopsis, radix, english, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tell us your story how did you become interes...</td>\n",
       "      <td>tell us your story how did you become interest...</td>\n",
       "      <td>[tell, us, your, story, how, did, you, become,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[please make sure you enter a valid and comple...</td>\n",
       "      <td>please make sure you enter a valid and complet...</td>\n",
       "      <td>[please, make, sure, you, enter, a, valid, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the hong kong automotive association and auto...</td>\n",
       "      <td>the hong kong automotive association and autos...</td>\n",
       "      <td>[the, hong, kong, automotive, association, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[electric vehicle charging stations are poppin...</td>\n",
       "      <td>electric vehicle charging stations are popping...</td>\n",
       "      <td>[electric, vehicle, charging, stations, are, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_list  \\\n",
       "0  [official name codonopsis radix english name t...   \n",
       "1  [tell us your story how did you become interes...   \n",
       "2  [please make sure you enter a valid and comple...   \n",
       "3  [the hong kong automotive association and auto...   \n",
       "4  [electric vehicle charging stations are poppin...   \n",
       "\n",
       "                                           text_full  \\\n",
       "0  official name codonopsis radix english name ta...   \n",
       "1  tell us your story how did you become interest...   \n",
       "2  please make sure you enter a valid and complet...   \n",
       "3  the hong kong automotive association and autos...   \n",
       "4  electric vehicle charging stations are popping...   \n",
       "\n",
       "                                          text_split  \n",
       "0  [official, name, codonopsis, radix, english, n...  \n",
       "1  [tell, us, your, story, how, did, you, become,...  \n",
       "2  [please, make, sure, you, enter, a, valid, and...  \n",
       "3  [the, hong, kong, automotive, association, and...  \n",
       "4  [electric, vehicle, charging, stations, are, p...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spaceseparate(s):\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "data[\"text_full\"] = data[\"text_split\"].apply(spaceseparate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the corpus:\t 2607\n",
      "Number of words in the corpus:\t\t 1699654\n",
      "Number of UNIQUE words in the corpus: \t 64525\n",
      "Mean document Length: \t\t\t 651.957805907173\n"
     ]
    }
   ],
   "source": [
    "documents = data[\"text_full\"].values\n",
    "documentsSplit = data[\"text_split\"].values\n",
    "\n",
    "words = []\n",
    "\n",
    "# Convert to one giant list\n",
    "for i in documentsSplit:\n",
    "    words.extend(i)\n",
    "    \n",
    "uniqueWords = set(words)\n",
    "\n",
    "print(\"Number of documents in the corpus:\\t\", len(data))\n",
    "print(\"Number of words in the corpus:\\t\\t\", len(words))\n",
    "print(\"Number of UNIQUE words in the corpus: \\t\", len(uniqueWords))\n",
    "print(\"Mean document Length: \\t\\t\\t\", len(words)/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation - Creating the Corpos\n",
    "\n",
    "In this notebook the data is further prepared. The main task is to identify the tokenisation of the words and lemmatise them.\n",
    "\n",
    "### Lemmatisation Examples:\n",
    "\n",
    "- Guns -> Gun\n",
    "- Swimming -> Swim\n",
    "- Swum -> Swim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos_tag the words, giving us tags that indicate \"noun\", \"verb\", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"text_tagged\"] = data[\"text_split\"].apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_list</th>\n",
       "      <th>text_full</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[official name codonopsis radix english name t...</td>\n",
       "      <td>official name codonopsis radix english name ta...</td>\n",
       "      <td>[official, name, codonopsis, radix, english, n...</td>\n",
       "      <td>[(official, JJ), (name, NN), (codonopsis, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tell us your story how did you become interes...</td>\n",
       "      <td>tell us your story how did you become interest...</td>\n",
       "      <td>[tell, us, your, story, how, did, you, become,...</td>\n",
       "      <td>[(tell, VB), (us, PRP), (your, PRP$), (story, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[please make sure you enter a valid and comple...</td>\n",
       "      <td>please make sure you enter a valid and complet...</td>\n",
       "      <td>[please, make, sure, you, enter, a, valid, and...</td>\n",
       "      <td>[(please, VB), (make, VB), (sure, JJ), (you, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the hong kong automotive association and auto...</td>\n",
       "      <td>the hong kong automotive association and autos...</td>\n",
       "      <td>[the, hong, kong, automotive, association, and...</td>\n",
       "      <td>[(the, DT), (hong, NN), (kong, RB), (automotiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[electric vehicle charging stations are poppin...</td>\n",
       "      <td>electric vehicle charging stations are popping...</td>\n",
       "      <td>[electric, vehicle, charging, stations, are, p...</td>\n",
       "      <td>[(electric, JJ), (vehicle, NN), (charging, VBG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_list  \\\n",
       "0  [official name codonopsis radix english name t...   \n",
       "1  [tell us your story how did you become interes...   \n",
       "2  [please make sure you enter a valid and comple...   \n",
       "3  [the hong kong automotive association and auto...   \n",
       "4  [electric vehicle charging stations are poppin...   \n",
       "\n",
       "                                           text_full  \\\n",
       "0  official name codonopsis radix english name ta...   \n",
       "1  tell us your story how did you become interest...   \n",
       "2  please make sure you enter a valid and complet...   \n",
       "3  the hong kong automotive association and autos...   \n",
       "4  electric vehicle charging stations are popping...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [official, name, codonopsis, radix, english, n...   \n",
       "1  [tell, us, your, story, how, did, you, become,...   \n",
       "2  [please, make, sure, you, enter, a, valid, and...   \n",
       "3  [the, hong, kong, automotive, association, and...   \n",
       "4  [electric, vehicle, charging, stations, are, p...   \n",
       "\n",
       "                                         text_tagged  \n",
       "0  [(official, JJ), (name, NN), (codonopsis, NN),...  \n",
       "1  [(tell, VB), (us, PRP), (your, PRP$), (story, ...  \n",
       "2  [(please, VB), (make, VB), (sure, JJ), (you, P...  \n",
       "3  [(the, DT), (hong, NN), (kong, RB), (automotiv...  \n",
       "4  [(electric, JJ), (vehicle, NN), (charging, VBG...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert the NLTK tokenisation to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a lemmatizer\n",
    "wnLem = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['official', 'name', 'codonopsis', 'radix', 'english', 'name', 'tangshen', 'scientific', 'name', 'codonpsis', 'pilosula', 'franch', 'nannf', 'codonopsis', 'pilosula', 'nannf', 'var', 'modesta', 'nannf', 'l', 't', 'shen', 'or', 'codonopsis', 'tangshen', 'oliv', 'chinese', 'read', 'more', 'official', 'name', 'cinnamomi', 'ramulus', 'english', 'name', 'cassia', 'twig', 'scientific', 'name', 'cinnamomum', 'cassia', 'presl', 'chinese', 'name', 'chinese', 'phonetic', 'name', 'gu', 'zh', 'official', 'name', 'angelicae', 'dahuricae', 'radix', 'english', 'name', 'dahurian', 'angelica', 'root', 'scientific', 'name', 'angelica', 'dahurica', 'fisch', 'ex', 'hoffm', 'benth', 'et', 'hook', 'f', 'angelica', 'dahurica', 'fisch', 'ex', 'hoffm', 'benth', 'read', 'more', 'official', 'name', 'fructus', 'forsythiae', 'english', 'name', 'weep', 'forsythia', 'capsule', 'scientific', 'name', 'forsythia', 'suspensa', 'thunb', 'vahl', 'chinese', 'name', 'chinese', 'phonetic', 'name', 'lian', 'qiao', 'source', 'the', 'dried', 'fruit', 'read', 'more', 'benefit', 'of', 'yunzhi', 'and', 'lingzhi', 'there', 'be', 'approximately', '100', 'thousand', 'know', 'mushroom', 'specie', 'in', 'the', 'world', 'which', 'have', 'be', 'a', 'rich', 'medicinal', 'reservoir', 'that', 'continue', 'today', 'read', 'more']\n"
     ]
    }
   ],
   "source": [
    "def lemmatise(s):\n",
    "    \"\"\"Lemmatization function that expects pos information (whether a word is verb, noun, etc.)\"\"\"\n",
    "    \"\"\"Input: [word, posInfo]\"\"\"\n",
    "    #print(s)\n",
    "    #print([x[0] for x in s])\n",
    "    \n",
    "    def lemWord(w):\n",
    "        tag = get_wordnet_pos(w[1])\n",
    "        if tag != None:\n",
    "            return wnLem.lemmatize(w[0], pos=tag)\n",
    "        else:\n",
    "            return wnLem.lemmatize(w[0])\n",
    "        \n",
    "\n",
    "    return [lemWord(x) for x in s]\n",
    "    \n",
    "\n",
    "for i in data.index[:1]:\n",
    "    print()\n",
    "    print(lemmatise(data.loc[i][\"text_tagged\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"text_lem\"] = data[\"text_tagged\"].apply(lemmatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only care about this column now\n",
    "corpus = data[[\"text_lem\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.to_csv(\"data/corpus_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for doc in data[\"text_lem\"]:\n",
    "    for word in doc:\n",
    "        frequency[word] += 1\n",
    "        \n",
    "filt = dict(frequency)\n",
    "\n",
    "f = sorted(filt.items(), key=lambda x: x[1])\n",
    "f.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3399"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency[\"health\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def filterSingletons(doc):\n",
    "    return [x for x in doc if frequency[x] > 1]\n",
    "\n",
    "corpus[\"text_lem\"] = corpus[\"text_lem\"].apply(filterSingletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[official, name, codonopsis, radix, english, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tell, u, your, story, how, do, you, become, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[please, make, sure, you, enter, a, valid, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, hong, kong, association, and, organize, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[electric, vehicle, charge, station, be, pop, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text_lem\n",
       "0  [official, name, codonopsis, radix, english, n...\n",
       "1  [tell, u, your, story, how, do, you, become, i...\n",
       "2  [please, make, sure, you, enter, a, valid, and...\n",
       "3  [the, hong, kong, association, and, organize, ...\n",
       "4  [electric, vehicle, charge, station, be, pop, ..."
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update frequencies\n",
    "frequency = defaultdict(int)\n",
    "for doc in corpus[\"text_lem\"]:\n",
    "    for word in doc:\n",
    "        frequency[word] += 1\n",
    "        \n",
    "filt = dict(frequency)\n",
    "\n",
    "f = sorted(filt.items(), key=lambda x: x[1])\n",
    "f.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in it be'.split())\n",
    "\n",
    "def filterStopWords(doc):\n",
    "    return [x for x in doc if x not in stoplist]\n",
    "\n",
    "corpus[\"text_lem\"] = corpus[\"text_lem\"].apply(filterStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(26867 unique tokens: ['stirfry', 'gmail', 'retirement', 'freaky', 'faulty']...)\n",
      "\n",
      "20246\n",
      "stirfry\n",
      "\n",
      "16149\n",
      "gmail\n",
      "\n",
      "7167\n",
      "retirement\n",
      "\n",
      "16699\n",
      "freaky\n",
      "\n",
      "20349\n",
      "faulty\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(corpus[\"text_lem\"].values)\n",
    "print(dictionary)\n",
    "for i in dictionary.keys()[:5]:\n",
    "    print()\n",
    "    print(i)\n",
    "    print(dictionary.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def vectorise(s):\n",
    "    return dictionary.doc2bow(s)\n",
    "    \n",
    "corpus[\"text_vec\"] = corpus[\"text_lem\"].apply(vectorise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The text_vec column is the actual corpus, words are encoded as in the dictionary\n",
    "corpora.MmCorpus.serialize('data/corpus.mm', corpus[\"text_vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('data/dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.to_csv(\"data/corpus_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lem</th>\n",
       "      <th>text_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[official, name, codonopsis, radix, english, n...</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 3), (3, 1), (4, 1), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tell, u, your, story, how, do, you, become, i...</td>\n",
       "      <td>[(4, 6), (5, 2), (26, 2), (34, 1), (39, 1), (5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[please, make, sure, you, enter, valid, comple...</td>\n",
       "      <td>[(74, 1), (139, 1), (147, 2), (179, 1), (268, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[hong, kong, association, organize, an, electr...</td>\n",
       "      <td>[(4, 1), (17, 1), (92, 1), (119, 1), (179, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[electric, vehicle, charge, station, pop, up, ...</td>\n",
       "      <td>[(17, 2), (26, 1), (39, 2), (79, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[traditional, chinese, medicine, tcm, have, ov...</td>\n",
       "      <td>[(4, 4), (6, 3), (26, 6), (34, 2), (44, 1), (4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[by, select, one, following, topic, you, can, ...</td>\n",
       "      <td>[(6, 4), (17, 1), (73, 1), (92, 1), (114, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[western, physiology, urine, consider, fluid, ...</td>\n",
       "      <td>[(4, 8), (5, 2), (6, 6), (17, 2), (21, 1), (23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[coriolus, versicolor, also, know, versicolor,...</td>\n",
       "      <td>[(4, 14), (5, 2), (6, 6), (18, 1), (26, 10), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[mushroom, health, supplement, that, combine, ...</td>\n",
       "      <td>[(4, 9), (5, 5), (6, 13), (18, 12), (26, 5), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[color, scheme, hyy, product, packaging, base,...</td>\n",
       "      <td>[(4, 4), (5, 1), (6, 4), (26, 1), (39, 1), (52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[an, ancillary, part, integrate, chinese, medi...</td>\n",
       "      <td>[(4, 5), (6, 13), (44, 3), (52, 12), (60, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[immune, system, require, balance, harmony, or...</td>\n",
       "      <td>[(52, 1), (106, 1), (193, 1), (403, 1), (423, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[lung, responsible, breathe, addition, this, c...</td>\n",
       "      <td>[(6, 1), (106, 1), (170, 1), (221, 1), (233, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[healthy, cardiovascular, system, ensure, oxyg...</td>\n",
       "      <td>[(6, 1), (106, 2), (170, 1), (221, 1), (233, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[chinese, medicine, believe, that, exercise, i...</td>\n",
       "      <td>[(6, 2), (52, 1), (92, 1), (105, 1), (119, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[liver, play, several, vital, function, body, ...</td>\n",
       "      <td>[(6, 1), (170, 1), (221, 1), (233, 1), (241, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[kidney, urinary, system, play, vital, excreto...</td>\n",
       "      <td>[(6, 1), (92, 1), (106, 2), (143, 1), (221, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[western, physiology, spleen, large, vascular,...</td>\n",
       "      <td>[(6, 2), (24, 1), (60, 1), (61, 1), (168, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[traditional, chinese, medicine, tcm, not, jus...</td>\n",
       "      <td>[(4, 4), (6, 5), (17, 1), (26, 2), (30, 1), (3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[accord, u, national, canine, cancer, foundati...</td>\n",
       "      <td>[(4, 32), (5, 1), (6, 7), (16, 2), (17, 7), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[accord, u, national, canine, cancer, foundati...</td>\n",
       "      <td>[(4, 9), (6, 1), (26, 1), (33, 1), (34, 1), (3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[integrate, chinese, medicine, holding, ltd, s...</td>\n",
       "      <td>[(4, 3), (6, 1), (26, 3), (52, 1), (60, 2), (8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[coriolus, versicolor, also, know, yunzhi, pre...</td>\n",
       "      <td>[(4, 7), (5, 1), (6, 2), (18, 1), (33, 1), (44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[make, from, unique, proprietary, ingredient, ...</td>\n",
       "      <td>[(4, 1), (52, 1), (60, 3), (66, 1), (82, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[imyunity, dog, wellresearched, mushroom, supp...</td>\n",
       "      <td>[(4, 1), (49, 5), (52, 1), (60, 2), (69, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[there, have, no, report, true, side, effect, ...</td>\n",
       "      <td>[(4, 13), (6, 3), (16, 2), (17, 6), (18, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[all, our, product, develop, by, professional,...</td>\n",
       "      <td>[(4, 2), (6, 1), (26, 1), (105, 1), (119, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[all, product, introduce, this, website, devel...</td>\n",
       "      <td>[(4, 7), (5, 2), (6, 16), (26, 2), (34, 2), (3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[only, distributor, list, on, contact, page, a...</td>\n",
       "      <td>[(4, 1), (6, 1), (26, 1), (130, 1), (135, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>[cnn, student, at, university, houston, new, m...</td>\n",
       "      <td>[(17, 1), (35, 1), (93, 1), (141, 1), (147, 2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>[cnnan, outbreak, salmonella, link, raw, turke...</td>\n",
       "      <td>[(4, 1), (17, 1), (35, 1), (93, 1), (137, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>[north, carolina, football, coach, larry, fedo...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>[teacher, on, flight, florida, visit, her, par...</td>\n",
       "      <td>[(17, 1), (35, 1), (89, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>[beirut, parliament, prepare, legalize, cannab...</td>\n",
       "      <td>[(17, 1), (22, 1), (33, 1), (35, 1), (93, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>[cnna, midwestern, grocery, store, chain, reca...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 1), (93, 1), (109, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>[cnna, mass, radio, campaign, burkina, faso, e...</td>\n",
       "      <td>[(4, 1), (17, 1), (35, 1), (72, 1), (93, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>[one, his, first, major, act, act, director, u...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 2), (93, 1), (132, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>[spell, large, letter, on, chalkboard, classro...</td>\n",
       "      <td>[(17, 1), (35, 1), (79, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>[monday, website, national, guideline, which, ...</td>\n",
       "      <td>[(17, 1), (35, 1), (60, 1), (93, 1), (119, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>[you, health, expert, at, home, arent, you, yo...</td>\n",
       "      <td>[(26, 1), (87, 1), (89, 1), (147, 5), (179, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>[rate, from, liver, cancer, increase, 43, amer...</td>\n",
       "      <td>[(4, 1), (17, 1), (35, 1), (93, 1), (137, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>[cnn, massachusetts, supreme, court, rule, mon...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>[go, his, parent, help, with, what, he, think,...</td>\n",
       "      <td>[(17, 1), (35, 1), (89, 1), (93, 1), (108, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>[cnna, common, drug, use, control, blood, pres...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>[after, month, ask, you, take, plunge, now, yo...</td>\n",
       "      <td>[(4, 1), (17, 1), (35, 1), (93, 1), (101, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>[cnnthe, southern, indian, state, kerala, anno...</td>\n",
       "      <td>[(17, 1), (35, 1), (52, 1), (93, 1), (128, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>[might, able, blame, heat, our, bad, decision,...</td>\n",
       "      <td>[(4, 1), (17, 1), (35, 1), (52, 1), (93, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>[20, living, with, type, 1, diabetes, mean, de...</td>\n",
       "      <td>[(17, 1), (35, 1), (93, 1), (118, 1), (119, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>[cnn, how, often, do, you, think, how, differe...</td>\n",
       "      <td>[(5, 2), (17, 1), (35, 1), (89, 1), (93, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>[people, with, blood, pressure, have, more, ma...</td>\n",
       "      <td>[(4, 1), (17, 1), (26, 1), (130, 1), (137, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>[cnnthe, british, association, sexual, health,...</td>\n",
       "      <td>[(4, 1), (5, 1), (17, 1), (26, 1), (35, 1), (9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>[cnnwhen, hospital, call, think, might, just, ...</td>\n",
       "      <td>[(4, 2), (17, 2), (35, 1), (52, 1), (87, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>[cnn, color, bubble, gum, flamingo, cotton, ca...</td>\n",
       "      <td>[(17, 1), (35, 1), (39, 1), (93, 1), (141, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>[first, time, compulsive, sexual, behavior, co...</td>\n",
       "      <td>[(17, 1), (35, 1), (39, 1), (82, 1), (93, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>[cnnin, an, experiment, with, global, implicat...</td>\n",
       "      <td>[(4, 1), (17, 2), (35, 1), (93, 1), (119, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>[florida, cnnthe, word, that, save, crystal, c...</td>\n",
       "      <td>[(4, 1), (5, 1), (17, 1), (35, 1), (52, 1), (8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>[while, ageing, society, have, become, one, to...</td>\n",
       "      <td>[(4, 2), (17, 1), (34, 1), (35, 2), (39, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>[cnnan, outbreak, cyclospora, link, mcdonalds,...</td>\n",
       "      <td>[(4, 1), (17, 1), (22, 1), (35, 1), (93, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>[remain, have, find, turkey, which, believe, s...</td>\n",
       "      <td>[(4, 1), (5, 1), (17, 1), (35, 1), (60, 1), (9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2607 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_lem  \\\n",
       "0     [official, name, codonopsis, radix, english, n...   \n",
       "1     [tell, u, your, story, how, do, you, become, i...   \n",
       "2     [please, make, sure, you, enter, valid, comple...   \n",
       "3     [hong, kong, association, organize, an, electr...   \n",
       "4     [electric, vehicle, charge, station, pop, up, ...   \n",
       "5     [traditional, chinese, medicine, tcm, have, ov...   \n",
       "6     [by, select, one, following, topic, you, can, ...   \n",
       "7     [western, physiology, urine, consider, fluid, ...   \n",
       "8     [coriolus, versicolor, also, know, versicolor,...   \n",
       "9     [mushroom, health, supplement, that, combine, ...   \n",
       "10    [color, scheme, hyy, product, packaging, base,...   \n",
       "11    [an, ancillary, part, integrate, chinese, medi...   \n",
       "12    [immune, system, require, balance, harmony, or...   \n",
       "13    [lung, responsible, breathe, addition, this, c...   \n",
       "14    [healthy, cardiovascular, system, ensure, oxyg...   \n",
       "15    [chinese, medicine, believe, that, exercise, i...   \n",
       "16    [liver, play, several, vital, function, body, ...   \n",
       "17    [kidney, urinary, system, play, vital, excreto...   \n",
       "18    [western, physiology, spleen, large, vascular,...   \n",
       "19    [traditional, chinese, medicine, tcm, not, jus...   \n",
       "20    [accord, u, national, canine, cancer, foundati...   \n",
       "21    [accord, u, national, canine, cancer, foundati...   \n",
       "22    [integrate, chinese, medicine, holding, ltd, s...   \n",
       "23    [coriolus, versicolor, also, know, yunzhi, pre...   \n",
       "24    [make, from, unique, proprietary, ingredient, ...   \n",
       "25    [imyunity, dog, wellresearched, mushroom, supp...   \n",
       "26    [there, have, no, report, true, side, effect, ...   \n",
       "27    [all, our, product, develop, by, professional,...   \n",
       "28    [all, product, introduce, this, website, devel...   \n",
       "29    [only, distributor, list, on, contact, page, a...   \n",
       "...                                                 ...   \n",
       "2578  [cnn, student, at, university, houston, new, m...   \n",
       "2579  [cnnan, outbreak, salmonella, link, raw, turke...   \n",
       "2580  [north, carolina, football, coach, larry, fedo...   \n",
       "2581  [teacher, on, flight, florida, visit, her, par...   \n",
       "2582  [beirut, parliament, prepare, legalize, cannab...   \n",
       "2583  [cnna, midwestern, grocery, store, chain, reca...   \n",
       "2584  [cnna, mass, radio, campaign, burkina, faso, e...   \n",
       "2585  [one, his, first, major, act, act, director, u...   \n",
       "2586  [spell, large, letter, on, chalkboard, classro...   \n",
       "2587  [monday, website, national, guideline, which, ...   \n",
       "2588  [you, health, expert, at, home, arent, you, yo...   \n",
       "2589  [rate, from, liver, cancer, increase, 43, amer...   \n",
       "2590  [cnn, massachusetts, supreme, court, rule, mon...   \n",
       "2591  [go, his, parent, help, with, what, he, think,...   \n",
       "2592  [cnna, common, drug, use, control, blood, pres...   \n",
       "2593  [after, month, ask, you, take, plunge, now, yo...   \n",
       "2594  [cnnthe, southern, indian, state, kerala, anno...   \n",
       "2595  [might, able, blame, heat, our, bad, decision,...   \n",
       "2596  [20, living, with, type, 1, diabetes, mean, de...   \n",
       "2597  [cnn, how, often, do, you, think, how, differe...   \n",
       "2598  [people, with, blood, pressure, have, more, ma...   \n",
       "2599  [cnnthe, british, association, sexual, health,...   \n",
       "2600  [cnnwhen, hospital, call, think, might, just, ...   \n",
       "2601  [cnn, color, bubble, gum, flamingo, cotton, ca...   \n",
       "2602  [first, time, compulsive, sexual, behavior, co...   \n",
       "2603  [cnnin, an, experiment, with, global, implicat...   \n",
       "2604  [florida, cnnthe, word, that, save, crystal, c...   \n",
       "2605  [while, ageing, society, have, become, one, to...   \n",
       "2606  [cnnan, outbreak, cyclospora, link, mcdonalds,...   \n",
       "2607  [remain, have, find, turkey, which, believe, s...   \n",
       "\n",
       "                                               text_vec  \n",
       "0     [(0, 1), (1, 1), (2, 3), (3, 1), (4, 1), (5, 1...  \n",
       "1     [(4, 6), (5, 2), (26, 2), (34, 1), (39, 1), (5...  \n",
       "2     [(74, 1), (139, 1), (147, 2), (179, 1), (268, ...  \n",
       "3     [(4, 1), (17, 1), (92, 1), (119, 1), (179, 1),...  \n",
       "4     [(17, 2), (26, 1), (39, 2), (79, 1), (141, 1),...  \n",
       "5     [(4, 4), (6, 3), (26, 6), (34, 2), (44, 1), (4...  \n",
       "6     [(6, 4), (17, 1), (73, 1), (92, 1), (114, 1), ...  \n",
       "7     [(4, 8), (5, 2), (6, 6), (17, 2), (21, 1), (23...  \n",
       "8     [(4, 14), (5, 2), (6, 6), (18, 1), (26, 10), (...  \n",
       "9     [(4, 9), (5, 5), (6, 13), (18, 12), (26, 5), (...  \n",
       "10    [(4, 4), (5, 1), (6, 4), (26, 1), (39, 1), (52...  \n",
       "11    [(4, 5), (6, 13), (44, 3), (52, 12), (60, 2), ...  \n",
       "12    [(52, 1), (106, 1), (193, 1), (403, 1), (423, ...  \n",
       "13    [(6, 1), (106, 1), (170, 1), (221, 1), (233, 1...  \n",
       "14    [(6, 1), (106, 2), (170, 1), (221, 1), (233, 1...  \n",
       "15    [(6, 2), (52, 1), (92, 1), (105, 1), (119, 1),...  \n",
       "16    [(6, 1), (170, 1), (221, 1), (233, 1), (241, 1...  \n",
       "17    [(6, 1), (92, 1), (106, 2), (143, 1), (221, 1)...  \n",
       "18    [(6, 2), (24, 1), (60, 1), (61, 1), (168, 1), ...  \n",
       "19    [(4, 4), (6, 5), (17, 1), (26, 2), (30, 1), (3...  \n",
       "20    [(4, 32), (5, 1), (6, 7), (16, 2), (17, 7), (1...  \n",
       "21    [(4, 9), (6, 1), (26, 1), (33, 1), (34, 1), (3...  \n",
       "22    [(4, 3), (6, 1), (26, 3), (52, 1), (60, 2), (8...  \n",
       "23    [(4, 7), (5, 1), (6, 2), (18, 1), (33, 1), (44...  \n",
       "24    [(4, 1), (52, 1), (60, 3), (66, 1), (82, 1), (...  \n",
       "25    [(4, 1), (49, 5), (52, 1), (60, 2), (69, 2), (...  \n",
       "26    [(4, 13), (6, 3), (16, 2), (17, 6), (18, 1), (...  \n",
       "27    [(4, 2), (6, 1), (26, 1), (105, 1), (119, 1), ...  \n",
       "28    [(4, 7), (5, 2), (6, 16), (26, 2), (34, 2), (3...  \n",
       "29    [(4, 1), (6, 1), (26, 1), (130, 1), (135, 1), ...  \n",
       "...                                                 ...  \n",
       "2578  [(17, 1), (35, 1), (93, 1), (141, 1), (147, 2)...  \n",
       "2579  [(4, 1), (17, 1), (35, 1), (93, 1), (137, 1), ...  \n",
       "2580  [(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...  \n",
       "2581  [(17, 1), (35, 1), (89, 1), (93, 1), (141, 1),...  \n",
       "2582  [(17, 1), (22, 1), (33, 1), (35, 1), (93, 1), ...  \n",
       "2583  [(17, 1), (35, 1), (52, 1), (93, 1), (109, 1),...  \n",
       "2584  [(4, 1), (17, 1), (35, 1), (72, 1), (93, 1), (...  \n",
       "2585  [(17, 1), (35, 1), (52, 2), (93, 1), (132, 1),...  \n",
       "2586  [(17, 1), (35, 1), (79, 1), (93, 1), (141, 1),...  \n",
       "2587  [(17, 1), (35, 1), (60, 1), (93, 1), (119, 1),...  \n",
       "2588  [(26, 1), (87, 1), (89, 1), (147, 5), (179, 1)...  \n",
       "2589  [(4, 1), (17, 1), (35, 1), (93, 1), (137, 1), ...  \n",
       "2590  [(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...  \n",
       "2591  [(17, 1), (35, 1), (89, 1), (93, 1), (108, 1),...  \n",
       "2592  [(17, 1), (35, 1), (52, 1), (93, 1), (141, 1),...  \n",
       "2593  [(4, 1), (17, 1), (35, 1), (93, 1), (101, 1), ...  \n",
       "2594  [(17, 1), (35, 1), (52, 1), (93, 1), (128, 1),...  \n",
       "2595  [(4, 1), (17, 1), (35, 1), (52, 1), (93, 1), (...  \n",
       "2596  [(17, 1), (35, 1), (93, 1), (118, 1), (119, 1)...  \n",
       "2597  [(5, 2), (17, 1), (35, 1), (89, 1), (93, 1), (...  \n",
       "2598  [(4, 1), (17, 1), (26, 1), (130, 1), (137, 1),...  \n",
       "2599  [(4, 1), (5, 1), (17, 1), (26, 1), (35, 1), (9...  \n",
       "2600  [(4, 2), (17, 2), (35, 1), (52, 1), (87, 1), (...  \n",
       "2601  [(17, 1), (35, 1), (39, 1), (93, 1), (141, 1),...  \n",
       "2602  [(17, 1), (35, 1), (39, 1), (82, 1), (93, 1), ...  \n",
       "2603  [(4, 1), (17, 2), (35, 1), (93, 1), (119, 1), ...  \n",
       "2604  [(4, 1), (5, 1), (17, 1), (35, 1), (52, 1), (8...  \n",
       "2605  [(4, 2), (17, 1), (34, 1), (35, 2), (39, 1), (...  \n",
       "2606  [(4, 1), (17, 1), (22, 1), (35, 1), (93, 1), (...  \n",
       "2607  [(4, 1), (5, 1), (17, 1), (35, 1), (60, 1), (9...  \n",
       "\n",
       "[2607 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
